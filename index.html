<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Image Matching: Structure from Motion</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <style>
    table {
        width: 75%;
        border-collapse: collapse; /* Collapse borders into a single border */
        border: 1px solid #000; /* Border around the table */
    }
    th, td {
        border: 1px solid #000; /* Border for table cells */
        padding: 8px; /* Padding inside cells for content */
        text-align: left; /* Align text to the left within cells */
    }
    th {
        background-color: #f2f2f2; /* Background color for table header cells */
    }
</style>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Image Matching: Structure from Motion</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/bhavaniam/" target="_blank">Bhavani A Madhabhavi</a>
                <!-- <sup>*</sup>, -->
              ,</span>
                <span class="author-block">
                  <a href="https://www.linkedin.com/in/akshaykiranjose/" target="_blank">Akshay Kiran Jose</a>
                  <!-- <sup>*</sup>, -->
                </span>
                  <!-- <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span> -->
                  </div>

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/BhavaniAM/bhv_akj_sfm.github.io" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Project</span>
                  </a>
                </span>

                <!-- Kaggle link -->
                  <span class="link-block">
                    <a href="https://www.kaggle.com/code/bhavaniam/imc-2024-sfm" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-kaggle"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
         We devise/improve upon an algorithm that estimates a three-dimensional structure from two-dimensional image sequences. 
         Multiple photographs of an object taken from different angles and at varying distances are analyzed and matched to estimate 
         the 3D structure of the object. Matching many images across different viewpoints to reconstruct a 3D model of an environment 
         from a collection of images is a computer vision problem known as <b> Structure from Motion (SfM).</b>
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
             The typical steps that are involved in estimating structure 
             from motion are: i) extraction of image embeddings, ii) key point detection 
             and matching, iii) rotation correction (if needed), iv) triangulation and 
             v) bundle adjustment. We explore model ensembling techniques for image 
             embedding extraction and for keypoint detection and matching in order to 
             utilize the capabilities of the models to capture the various details in 
             an image while improving the overall accuracy of reconstruction. We experiment 
             with different models and hyperparameters and orientation correction to obtain 
             the best combination for SfM. Since this algorithm was devised as part of 
             the <u><a href="https://www.kaggle.com/competitions/image-matching-challenge-2024/overview" style="color:#0800ff;" target="_blank">Kaggle Image Matching Challenge 2024</a></u>, the code executes within 9 hours on 
             a GPU and was optimized to obtain the highest accuracy for the task among the 
             many variations tested.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <center><img src="static/images/IMC_2024_github_io.png" alt="MY ALT TEXT"/></center>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <!--<div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <img src="static/images/00001.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <img src="static/images/00010.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
        </h2>
      </div>
      <div class="item">
        <img src="static/images/00030.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
       </h2>
     </div>
     <div class="item">
      <img src="static/images/00042.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
    <div class="item">
      <img src="static/images/00066.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
    <div class="item">
      <img src="static/images/00111.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
      </h2>
    </div>
  </div> -->
</div>
</div>
</section>
<!-- End image carousel -->


<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Dataset</h2>
      <!-- <div class="columns"> -->
        <!-- <div class="column is-four-fifths"> -->
          The training data contains different classes with their respective images and the 3-D reconstructions
          for the images, which can be opened with colmap. The training labels are: 
          i) dataset, ii) scene, iii) image_path, iv) rotation_matrix and v) the translation_vector. 
          The test data contains only the images for each class. The inferencing of the model is done 
          on the test dataset. 
          <br>
          The images displayed in the above section are examples taken from one of the classes in the test data. 
          The multiple images taken from various viewpoints are used to reconstruct the 3D model of the actual 
          object and to accurately pose (obtain the rotation matrix and translation vector of the camera's position) 
          the images. There are two resolutions of images present in the dataset: 1024 x 768 pixels (landscape) 
          and 768 x 1024 pixels (portrait). 

          <!-- <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div> -->
        <!-- </div> -->
      <!-- </div> -->
    </div>
  </div>
</section>
<!-- End youtube video -->


<!-- Video carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Algorithm</h2>
      <h3><b>1) Feature Extraction and finding similar image pairs</b></h3>
      The purpose of the task is to compute global descriptors (feature representations such as image embeddings) for images using a deep learning model, 
      then generate pairs of images based on the similarity of these descriptors. We use a pre-trained deep learning model (tf_efficientnet_b7) to extract
      features from the input images. The descriptors are normalized using L2 normalization to ensure that each descriptor vector has a unit norm, which 
      can improve the robustness and effectiveness of subsequent similarity calculations. The pairwise distances are computed on these descriptors to 
      extract pairs of images that are most similar to each other. 
      <br>
      <br>
      <h3><b>2) Keypoint detection</b></h3>
      Keypoints are spatial locations in an image that represent regions of an image that are unique and can be robustly matched across different images 
      under various transformations such as changes in viewpoint, scale, rotation, and illumination. We want to identify these informative points within 
      an image as they serve as landmarks or reference points to establish correspondences between different images for image matching. We match 
      keypoints and descriptors between pairs of images using the LightGlue algorithm (ALIKED).
      <br>
      <br>
      <h3><b>3) Keypoint merger</b></h3>
      Combining keypoints and matches from multiple images or sources into a unified dataset is necessary when using multiple keypoint detection and 
      matching algorithms as each algorithm may detect different sets of keypoints due to variations in their detection methods, sensitivity to 
      different image features, and robustness to different image conditions. By merging keypoints and matches from multiple algorithms or datasets, 
      we obtain a more complete representation of the scene or object being analyzed. Since our algorithm uses DoGHardNet and ALIKED, we include a 
      keypoint merger. 
      <br>
      <br>
      <h3><b>4) Calculate the fundamental matrix</b></h3>
      Matches between keypoints can contain outliers due to factors like noise, occlusion, or mismatches. We employ RANSAC to remove the outliers 
      between the keypoint matches. RANSAC randomly selects a minimal subset of matches to estimate the initial matrix parameters. All the matches 
      are then evaluated against the estimated matrix and are classified as inliers and outliers. After several iterations, RANSAC selects the matrix 
      with the highest number of inliers. This matrix is then refined using all inlier matches to obtain a more accurate estimate of the transformation 
      parameters. The final matrix is known as the fundamental matrix.
      <br>
      <br>
      <h3><b>5) Bundle adjustment</b></h3>
      Upon obtaining the keypoint matches without outliers, we use pycolmap, which offers an incremental reconstruction algorithm that starts from two 
      pairs of images and continually adds more and more images to the scene, resulting in a reconstructed scene with camera information (translation 
      vector and rotation matrix). 

      We further perform bundle adjustment to optimize camera parameters and 3D point positions based on observed image data to refine the 3D 
      reconstructed models. Its primary goal is to minimize the reprojection error, which is the difference between the observed 2D keypoints in images 
      and their corresponding projections from the estimated 3D structure.


      <!-- <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div> -->
    </div>
  </div>
</section>
<!-- End video carousel -->


<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Experiments</h2>
      We performed multiple experiments on the algorithm such as model ensembling, orientation detection and hyperparameter tuning.
      We detail the change in performance of the overall algorithm on the held out testset for every experiment in the 
      table below with the corresponding changes to the model/algorithm.
      
      <table>
        <thead>
            <tr>
                <th><center>Method</center></th>
                <th>Score obtained</th>
            </tr>
        </thead>
        <tbody>
            <tr>
                <td>Baseline (DinoV2. Image size 1024. ALIKED. 4196 features)</td>
                <td>0.119</td>
            </tr>
            <tr>
                <td>Ensemble of Dinov2, Swinv2 tiny, ViT. Image size 1024 &rarr; 1280. ALIKED. 4196 features</td>
                <td>0.129</td>
            </tr>
            <tr>
                <td>Ensemble of Dinov2, Swinv2 tiny, ViT. Image size 1280. ALIKED. Features 4196 &rarr; 8192 </td>
                <td>0.131</td>
            </tr>
            <tr>
              <td>tf_efficientnet_b7. Image size 1280. ALIKED. 8192 features </td>
              <td>0.139</td>
            </tr>
            <tr>
              <td>tf_efficientnet_b7. Image size 1280 &rarr; 1600. ALIKED + DoGHardNet ensemble. 8192 features</td>
              <td>0.147</td>
            </tr>
            <tr>
              <td>tf_efficientnet_b7. Image size 1600 &rarr; 1920. ALIKED + DoGHardNet ensemble. 8192 features. min_matches 15 &rarr; 10</td>
              <td>0.161</td>
            </tr>
        </tbody>
    </table>
    <br>
    <b>Baseline:</b> The baseline algorithm uses a single model to extract image embeddings (DinoV2) and a single model for 
    feature extraction (LightGlue ALIKED). 
    <br>
    <br>
    <b>An increase in the image size:</b> Although a higher image resolution improves the score of the algorithm, it affects 
    the runtime and the complexity of our model (more multiplications to run the same model for a higher image resolution). 
    <br>
    <br>
    <b>More features considered in keypoint matching:</b> Increasing the number of features that are to be detected in the images 
    naturally increases the runtime of the algorithm. However, it does make the algorithm robust since we are increasing the 
    number of unique points that are being considered in each image, which makes it easier for the network to identify the 
    said object/location in the image.
    <br>
    <br>
    <b>Ensembling:</b> We experiment with/utilize model ensembling to extract image embeddings and for keypoint detection. While one 
    model might not be able to effectively capture the nuances in the images, using multiple models ensures that we leverage 
    the independent capabilities of the models and combine them to capture the details from the images, necessary for the task. 
    </div>
  </div>
</section>



<!-- Paper poster -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Conclusion</h2>
        To summarize, we work on improving a solution to the structure-from-motion problem to reconstruct 
        3D models from multiple 2D images that are taken from various viewpoints and in different settings. 

      <!-- <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe> -->
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


<!--   <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
